//go:generate go run . -out ../../asm_amd64.s -stubs ../../stubs_amd64.go -pkg vectorcmp --dispatcher-amd64 ../../dispatch_amd64.go --dispatcher-other ../../dispatch_other.go --generate-test ../../generated_test.go
package main

import (
	"bytes"
	"flag"
	"fmt"
	"go/format"
	"io"
	"log"
	"os"
	"strings"

	. "github.com/mmcloughlin/avo/build"
	"github.com/mmcloughlin/avo/operand"
	. "github.com/mmcloughlin/avo/operand"
	"github.com/mmcloughlin/avo/reg"
)

var (
	constants                = GLOBL("constants", RODATA|NOPTR)
	const_zeroes             Mem
	const_onezero            Mem
	const_three_through_zero Mem
	const_seven_through_zero Mem
	const_drop_half          Mem
	const_drop_threequarters Mem
	const_drop_seveneight    Mem
)

var (
	dispatcherAmd64File = flag.String("dispatcher-amd64", "", "File name to write dispatcher functions for amd64 to")
	dispatcherOtherFile = flag.String("dispatcher-other", "", "File name to write dispatcher functions for non-amd64 to")
	generatedTestFile   = flag.String("generate-test", "", "File name to write a generated test to")

	generatedTest   bytes.Buffer
	dispatcherAmd64 bytes.Buffer
	dispatcherOther bytes.Buffer
	dispatcherBoth  = io.MultiWriter(&dispatcherAmd64, &dispatcherOther)
)

type CmpOp string

var (
	Equals        CmpOp = "Equals"
	NotEquals     CmpOp = "NotEquals"
	GreaterThan   CmpOp = "GreaterThan"
	LessThan      CmpOp = "LessThan"
	GreaterEquals CmpOp = "GreaterEquals"
	LesserEquals  CmpOp = "LesserEquals"
	IsNaN         CmpOp = "IsNaN"
)

type AVXLevel string

var (
	AVX  AVXLevel = "AVX"
	AVX2 AVXLevel = "AVX2"
)

func (a AVXLevel) Bits() int {
	switch a {
	case AVX:
		return 128
	case AVX2:
		return 256
	default:
		panic("invalid level")
	}
}

type IsFloating bool

func (f IsFloating) GoType(bits int) string {
	if f {
		return fmt.Sprintf("float%d", bits)
	}
	return fmt.Sprintf("uint%d", bits)
}

func (f IsFloating) GoName(bits int) string {
	t := f.GoType(bits)
	return strings.ToUpper(t[:1]) + t[1:]
}

func main() {
	DATA(0, U32(0b01010101010101010101010101010101))
	DATA(4, U32(0b00010001000100010001000100010001))
	DATA(8, U32(0b00000001000000010000000100000001))

	const_zeroes = GLOBL("const_zeroes", RODATA|NOPTR)
	for i := 0; 16 > i; i += 4 {
		DATA(i, U32(0))
	}
	const_onezero = GLOBL("const_onezero", RODATA|NOPTR)
	for i := 0; 16 > i; i += 4 {
		DATA(i, U32(0x01000100))
	}
	const_three_through_zero = GLOBL("const_three_through_zero", RODATA|NOPTR)
	for i := 0; 16 > i; i += 4 {
		DATA(i, U32(0x03020100))
	}
	const_seven_through_zero = GLOBL("const_seven_through_zero", RODATA|NOPTR)
	for i := 0; 16 > i; i += 8 {
		DATA(i, U64(0x0706050403020100))
	}
	const_drop_half = GLOBL("const_drop_half", RODATA|NOPTR)
	DATA(0, U64(0x0e0c0a0806040200))
	DATA(8, U64(0xffffffffffffffff))
	const_drop_threequarters = GLOBL("const_drop_threequarters", RODATA|NOPTR)
	DATA(0, U64(0xffffffff0c080400))
	DATA(8, U64(0xffffffffffffffff))
	const_drop_seveneight = GLOBL("const_drop_seveneight", RODATA|NOPTR)
	DATA(0, U64(0xffffffffffff0800))
	DATA(8, U64(0xffffffffffffffff))

	ConstraintExpr("!purego")

	io.WriteString(dispatcherBoth, "// Code generated by avo.go. DO NOT EDIT.\n\n")
	dispatcherAmd64.WriteString("//go:build amd64 && !purego\n\n")
	dispatcherOther.WriteString("//go:build !amd64 || purego\n\n")
	io.WriteString(dispatcherBoth, "package vectorcmp\n\n")

	io.WriteString(&generatedTest, "// Code generated by avo.go. DO NOT EDIT.\n\n")
	io.WriteString(&generatedTest, "package vectorcmp\n\n")
	io.WriteString(&generatedTest, "import \"testing\"\n\n")
	io.WriteString(&generatedTest, "import \"bytes\"\n\n")

	for _, w := range []int{8, 16, 32, 64} {
		for _, o := range []CmpOp{Equals, NotEquals, GreaterThan, LessThan, GreaterEquals, LesserEquals} {
			fastFilter(w, o, false)
			if w >= 32 {
				fastFilter(w, o, true)
			}
		}
	}
	fastFilter(32, IsNaN, true)
	fastFilter(64, IsNaN, true)

	Generate()

	for _, d := range []struct {
		fn  string
		src *bytes.Buffer
	}{{*dispatcherAmd64File, &dispatcherAmd64}, {*dispatcherOtherFile, &dispatcherOther}, {*generatedTestFile, &generatedTest}} {
		if d.fn == "" {
			continue
		}
		src, err := format.Source(d.src.Bytes())
		if err != nil {
			log.Fatalf("Failed to format generated Go code: %v", err)
		}
		if err := os.WriteFile(d.fn, src, 0644); err != nil {
			log.Fatalf("Failed to write to %q: %v", d.fn, err)
		}
	}
}

type GPOp interface {
	Op
	reg.GP
}

func fastFilter(width int, cmpOp CmpOp, isfp IsFloating) {
	var infix string
	if isfp {
		infix = "Float"
	}
	rounds := 2
	if width == 64 {
		rounds = 4
	}
	defB := fmt.Sprintf("b %s, ", isfp.GoType(width))
	useB := "b, "
	testB := "'b', "
	if cmpOp == IsNaN {
		defB = ""
		useB = ""
		testB = ""
	}
	fmt.Fprintf(dispatcherBoth, "func Vector%s%s%d(dstMask []byte, %srows []%s) {\n", cmpOp, infix, width, defB, isfp.GoType(width))
	fmt.Fprintf(&dispatcherAmd64, "	if hasAVX2AndBMI2() && len(rows) >= %d {\n", 256*rounds/width)
	fmt.Fprintf(&dispatcherAmd64, "		boundsCheck(dstMask, rows)\n")
	fmt.Fprintf(&dispatcherAmd64, "		asmAVX2%s%s(dstMask, %srows[:len(rows) & ^%d])\n", cmpOp, isfp.GoName(width), useB, 256*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "		dstMask = dstMask[(len(rows) & ^%d) / 8:]\n", 256*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "		rows = rows[len(rows) & ^%d:]\n", 256*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "	} else if hasAVX() && len(rows) >= %d {\n", 128*rounds/width)
	fmt.Fprintf(&dispatcherAmd64, "		boundsCheck(dstMask, rows)\n")
	fmt.Fprintf(&dispatcherAmd64, "		asmAVX%s%s(dstMask, %srows[:len(rows) & ^%d])\n", cmpOp, isfp.GoName(width), useB, 128*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "		dstMask = dstMask[(len(rows) & ^%d) / 8:]\n", 128*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "		rows = rows[len(rows) & ^%d:]\n", 128*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "	}\n")
	fmt.Fprintf(dispatcherBoth, "	goVector%s(dstMask, %srows)\n", cmpOp, useB)
	fmt.Fprintf(dispatcherBoth, "}\n")

	fastFilterImpl(AVX2, width, cmpOp, isfp, rounds)
	fastFilterImpl(AVX, width, cmpOp, isfp, rounds)

	fmt.Fprintf(&generatedTest, "func TestVector%s%s%d(t *testing.T) {\n", cmpOp, infix, width)
	fmt.Fprintf(&generatedTest, "	t.Parallel()\n")
	fmt.Fprintf(&generatedTest, "	buf := randomBuffer[%s]()\n", isfp.GoType(width))
	fmt.Fprintf(&generatedTest, "	got := destinationBuffer(buf)\n")
	fmt.Fprintf(&generatedTest, "	want := destinationBuffer(buf)\n")
	fmt.Fprintf(&generatedTest, "	Vector%s%s%d(want, %sbuf)\n", cmpOp, infix, width, testB)
	fmt.Fprintf(&generatedTest, "	goVector%s(got, %sbuf)\n", cmpOp, testB)
	fmt.Fprintf(&generatedTest, "	if !bytes.Equal(want, got) {\n")
	fmt.Fprintf(&generatedTest, "		t.Fatalf(\"ASM version returned a different result:\\n%%b\\n%%b\", want, got)\n")
	fmt.Fprintf(&generatedTest, "	}\n")
	fmt.Fprintf(&generatedTest, "}\n")

	fmt.Fprintf(&generatedTest, "func BenchmarkAsmVector%s%s%d(b *testing.B) {\n", cmpOp, infix, width)
	fmt.Fprintf(&generatedTest, "	if !hasAVX2AndBMI2() && !hasAVX() {\n")
	fmt.Fprintf(&generatedTest, "		b.Skip(\"Both AVX and AVX2 are unavailable\")\n")
	fmt.Fprintf(&generatedTest, "	}\n")
	fmt.Fprintf(&generatedTest, "	buf := randomBuffer[%s]()\n", isfp.GoType(width))
	fmt.Fprintf(&generatedTest, "	dst := destinationBuffer(buf)\n")
	fmt.Fprintf(&generatedTest, "	b.ResetTimer()\n")
	fmt.Fprintf(&generatedTest, "	for i := 0; b.N > i; i++ {\n")
	fmt.Fprintf(&generatedTest, "		Vector%s%s%d(dst, %sbuf)\n", cmpOp, infix, width, testB)
	fmt.Fprintf(&generatedTest, "	}\n")
	fmt.Fprintf(&generatedTest, "}\n")

	fmt.Fprintf(&generatedTest, "func BenchmarkGoVector%s%s%d(b *testing.B) {\n", cmpOp, infix, width)
	fmt.Fprintf(&generatedTest, "	buf := randomBuffer[%s]()\n", isfp.GoType(width))
	fmt.Fprintf(&generatedTest, "	dst := destinationBuffer(buf)\n")
	fmt.Fprintf(&generatedTest, "	b.ResetTimer()\n")
	fmt.Fprintf(&generatedTest, "	for i := 0; b.N > i; i++ {\n")
	fmt.Fprintf(&generatedTest, "		goVector%s(dst, %sbuf)\n", cmpOp, testB)
	fmt.Fprintf(&generatedTest, "	}\n")
	fmt.Fprintf(&generatedTest, "}\n")
}

func fastFilterImpl(avxLevel AVXLevel, width int, cmpOp CmpOp, isfp IsFloating, rounds int) {
	useBMI2 := avxLevel == AVX2
	defB := fmt.Sprintf("b %s, ", isfp.GoType(width))
	if cmpOp == IsNaN {
		defB = ""
	}
	TEXT(fmt.Sprintf("asm%s%s%s", avxLevel, cmpOp, isfp.GoName(width)), NOSPLIT, fmt.Sprintf("func(dstMask []byte, %srows []%s)", defB, isfp.GoType(width)))
	Pragma("noescape")

	dstMask := Load(Param("dstMask").Base(), GP64())
	rows := Load(Param("rows").Base(), GP64())
	l := Load(Param("rows").Len(), GP64())

	var createVectorRegister func() reg.VecVirtual
	var vecRegName string
	switch avxLevel {
	case AVX:
		createVectorRegister = XMM
		vecRegName = "XMM"
	case AVX2:
		createVectorRegister = YMM
		vecRegName = "YMM"
	}

	bRepeated := createVectorRegister()
	var ymms []Op
	var intermediates []GPOp
	for i := 0; rounds > i; i++ {
		ymms = append(ymms, createVectorRegister())
		intermediates = append(intermediates, GP32())
	}

	if cmpOp != IsNaN {
		Commentf("Read param b into %s register. If b is 0x07, YMM becomes {0x07, 0x07, 0x07...}", vecRegName)
		b, err := Param("b").Resolve()
		if err != nil {
			panic(err)
		}
		switch avxLevel {
		case AVX2:
			switch width {
			case 8:
				VPBROADCASTB(b.Addr, bRepeated)
			case 16:
				VPBROADCASTW(b.Addr, bRepeated)
			case 32:
				VPBROADCASTD(b.Addr, bRepeated)
			case 64:
				VPBROADCASTQ(b.Addr, bRepeated)
			}
		case AVX:
			tmp := GP64()
			switch width {
			case 8:
				MOVB(b.Addr, tmp.As8())
				MOVQ(tmp, bRepeated)
				VPSHUFB(const_zeroes.Offset(0), bRepeated, bRepeated)
			case 16:
				MOVW(b.Addr, tmp.As16())
				MOVQ(tmp, bRepeated)
				VPSHUFB(const_onezero.Offset(0), bRepeated, bRepeated)
			case 32:
				MOVL(b.Addr, tmp.As32())
				MOVQ(tmp, bRepeated)
				VPSHUFB(const_three_through_zero.Offset(0), bRepeated, bRepeated)
			case 64:
				MOVQ(b.Addr, bRepeated)
				VPSHUFB(const_seven_through_zero.Offset(0), bRepeated, bRepeated)
			}
		}
	}

	mask := GP32()
	if useBMI2 {
		switch width {
		case 8:
		case 16:
			Comment("Load the mask 01010101... which we will use with PEXT to drop half the bits")
			MOVL(constants.Offset(0), mask)
		case 32:
			Comment("Load the mask 00010001... which we will use with PEXT to drop 75% of the bits")
			MOVL(constants.Offset(4), mask)
		case 64:
			Comment("Load the mask 00000001... which we will use with PEXT to drop 7/8th of the bits")
			MOVL(constants.Offset(8), mask)
		}
	}

	Label("loop")

	Commentf("Load %d %d-bit chunks into %s registers", rounds, avxLevel.Bits(), vecRegName)
	for i := 0; i < rounds; i++ {
		VMOVDQU(Mem{Base: rows, Disp: avxLevel.Bits() / 8 * i}, ymms[i])
	}
	Commentf("Compare all values in each %s register to b. Each byte in the YMMs becomes 0x00 (mismatch) or 0xff (match)", vecRegName)
	var compareAgainst operand.Op = bRepeated
	for i := 0; i < rounds; i++ {
		if isfp {
			var op U8
			switch cmpOp {
			case Equals:
				op = U8(0x00) // EQ_OQ: Equal (ordered, non-signaling)
			case NotEquals:
				op = U8(0x04) // NEQ_UQ: Not-equal (unordered, non-signaling)
			case LessThan:
				op = U8(0x11) // LT_OQ: Less-than (ordered, nonsignaling)
			case LesserEquals:
				op = U8(0x12) // LE_OQ: Less-than-or-equal (ordered, nonsignaling)
			case GreaterThan:
				op = U8(0x1E) // GT_OQ: Greater-than (ordered, nonsignaling)
			case GreaterEquals:
				op = U8(0x1D) // GE_OQ: Greater-than-or-equal (ordered, nonsignaling)
			case IsNaN:
				op = U8(0x04) // NEQ_OQ: Not-equal (unordered, non-signaling)
				compareAgainst = ymms[i]
			}
			if width == 32 {
				VCMPPS(op, ymms[i], compareAgainst, ymms[i])
			} else {
				VCMPPD(op, ymms[i], compareAgainst, ymms[i])
			}
			continue
		}
		var instr func(ops ...Op)
		switch width {
		case 8:
			if cmpOp == Equals || cmpOp == NotEquals {
				instr = VPCMPEQB
			} else {
				instr = VPCMPGTB
			}
		case 16:
			if cmpOp == Equals || cmpOp == NotEquals {
				instr = VPCMPEQW
			} else {
				instr = VPCMPGTW
			}
		case 32:
			if cmpOp == Equals || cmpOp == NotEquals {
				instr = VPCMPEQD
			} else {
				instr = VPCMPGTD
			}
		case 64:
			if cmpOp == Equals || cmpOp == NotEquals {
				instr = VPCMPEQQ
			} else {
				instr = VPCMPGTQ
			}
		}
		args := []Op{ymms[i], bRepeated, ymms[i]}
		switch cmpOp {
		case LessThan, GreaterEquals:
			// Flip the arguments
			args[0], args[1] = args[1], args[0]
		}
		instr(args...)
	}
	if !useBMI2 && width != 8 {
		var mask Mem
		switch width {
		case 16:
			Comment("Drop every second byte from these registers")
			mask = const_drop_half
		case 32:
			Comment("Drop every second-fourth byte from these registers")
			mask = const_drop_threequarters
		case 64:
			Comment("Drop every second-seventh byte from these registers")
			mask = const_drop_seveneight
		}
		for i := 0; i < rounds; i++ {
			VPSHUFB(mask.Offset(0), ymms[i], ymms[i])
		}
	}
	Comment("Take one bit of each byte and pack it into an R32")
	for i := 0; i < rounds; i++ {
		VPMOVMSKB(ymms[i], intermediates[i])
	}
	if useBMI2 {
		switch width {
		case 8:
		case 16:
			Comment("Drop every second bit from these registers")
			for i := 0; i < rounds; i++ {
				// TODO: We could avoid the PEXTL (which requires BMI2) with a VPSHUFB to first drop every second byte before calling VPMOVMSKB.
				PEXTL(mask, intermediates[i], intermediates[i])
			}
		case 32:
			Comment("Drop every second-fourth bit from these registers")
			for i := 0; i < rounds; i++ {
				PEXTL(mask, intermediates[i], intermediates[i])
			}
		case 64:
			Comment("Drop every second-seventh bit from these registers")
			for i := 0; i < rounds; i++ {
				PEXTL(mask, intermediates[i], intermediates[i])
			}
		}
	}
	resultBits := avxLevel.Bits() / width
	implementedWithInversion := (cmpOp == GreaterEquals || cmpOp == LesserEquals || cmpOp == NotEquals) && !isfp
	if implementedWithInversion {
		if cmpOp == NotEquals {
			Commentf("To get %s semantics we need to invert the result", cmpOp)
		} else {
			Commentf("To get %s semantics, we flipped the arguments of VPCMPGT and now invert the result", cmpOp)
		}
		for _, r := range intermediates {
			switch resultBits {
			case 32:
				NOTL(r)
			case 16:
				NOTW(r.As16())
			case 8:
				NOTB(r.As8())
			case 4, 2:
				// Handled below after combining them
			}
		}
	}
	switch resultBits {
	case 32, 16, 8:
		Comment("Write the registers to dstMask")
		for i, r := range intermediates {
			switch resultBits {
			case 32:
				MOVL(r, Mem{Base: dstMask, Disp: resultBits / 8 * i})
			case 16:
				MOVW(r.As16(), Mem{Base: dstMask, Disp: resultBits / 8 * i})
			case 8:
				MOVB(r.As8(), Mem{Base: dstMask, Disp: resultBits / 8 * i})
			}
		}
	case 4:
		Comment("Each register contains 4 bits, so we first combine pairs before writing them back")
		for i := 0; rounds/2 > i; i++ {
			SHLB(U8(4), intermediates[2*i+1].As8())
			ORB(intermediates[2*i+1].As8(), intermediates[2*i+0].As8())
			if implementedWithInversion {
				NOTB(intermediates[2*i+0].As8())
			}
			MOVB(intermediates[2*i+0].As8(), Mem{Base: dstMask, Disp: 1 * i})
		}
	case 2:
		Comment("Each register contains 2 bits, so we first combine them back into bytes before writing them back")
		for i := 0; rounds/4 > i; i++ {
			SHLB(U8(2), intermediates[4*i+1].As8())
			SHLB(U8(4), intermediates[4*i+2].As8())
			SHLB(U8(6), intermediates[4*i+3].As8())
			ORB(intermediates[4*i+1].As8(), intermediates[4*i+0].As8())
			ORB(intermediates[4*i+3].As8(), intermediates[4*i+2].As8())
			ORB(intermediates[4*i+2].As8(), intermediates[4*i+0].As8())
			if implementedWithInversion {
				NOTB(intermediates[4*i+0].As8())
			}
			MOVB(intermediates[4*i+0].As8(), Mem{Base: dstMask, Disp: 1 * i})
		}
	}

	Comment("Update our offsets into rows and dstMask")
	ADDQ(U32(rounds*avxLevel.Bits()/8), rows)
	ADDQ(U32(rounds*avxLevel.Bits()/8/width), dstMask)

	Comment("Decrement loop counter")
	SUBQ(U32(rounds*avxLevel.Bits()/width), l)
	JNZ(LabelRef("loop"))
	VZEROALL()
	RET()
}
